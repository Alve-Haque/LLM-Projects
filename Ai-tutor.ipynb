{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0816677c",
   "metadata": {},
   "source": [
    "# 1. Response from AI tutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abe804c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from scraper import fetch_website_links, fetch_website_contents\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a38acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key) > 10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a48b50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ea2c863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an Ai engineer, phd in mathematics, deep learning engineer, and expert in other topics\"\"\"\n",
    "system_prompt += \"\"\"\n",
    "When responding, always use Markdown for formatting. For any code, use well-structured code blocks with syntax highlighting,\n",
    "For instance:\n",
    "```python\n",
    "\n",
    "sample_list = [for i in range(10)]\n",
    "```\n",
    "\n",
    "Break down explanations into clear, numbered steps for better understanding. \n",
    "Highlight important terms using inline code formatting (e.g., `function_name`, `variable`).\n",
    "Provide examples for any concepts and ensure all examples are concise, clear, and relevant.\n",
    "Your goal is to create visually appealing, easy-to-read, and informative responses.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "791d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutor_user_prompt(question):\n",
    "    # Ensure the question is properly appended to the user prompt.\n",
    "    user_prompt = (\n",
    "        \"Please carefully explain the following question in a step-by-step manner:\\n\\n\"\n",
    "    )\n",
    "    user_prompt += question\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b395db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tutor(question, MODEL):\n",
    "    # Generate the user prompt dynamically.\n",
    "    user_prompt = tutor_user_prompt(question)\n",
    "    \n",
    "    # OpenAI API call to generate response.\n",
    "    if MODEL == 'gpt-4o-mini':\n",
    "        print(f'You are getting response from {MODEL}')\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            stream=True\n",
    "        )\n",
    "    else:\n",
    "        MODEL == 'llama3.2'\n",
    "        print(f'You are getting response from {MODEL}')\n",
    "        stream = ollama_via_openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "    # Initialize variables for response processing.\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    # Process the response stream and update display dynamically.\n",
    "    for chunk in stream:\n",
    "        # Safely access the content attribute.\n",
    "        response_chunk = getattr(chunk.choices[0].delta, \"content\", \"\")\n",
    "        if response_chunk:  # Check if response_chunk is not None or empty\n",
    "            response += response_chunk\n",
    "            # No replacement of Markdown formatting here!\n",
    "            update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "60268fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please answer the question and explain the answer:\n",
    "what is the capital of United states of america\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c539d4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are getting response from gpt-4o-mini\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure! Let's break down the question step-by-step.\n",
       "\n",
       "### Question:\n",
       "**What is the capital of the United States of America?**\n",
       "\n",
       "### Steps to Understand the Answer:\n",
       "\n",
       "1. **Identify the Subject Matter**:\n",
       "   - The question asks about the **capital city** of a specific country, in this case, the **United States of America (USA)**.\n",
       "\n",
       "2. **Understanding \"Capital\"**:\n",
       "   - A **capital** is defined as the city where the central government is located. It typically hosts government buildings, including the legislative, executive, and often the judicial branches.\n",
       "\n",
       "3. **Historical Context**:\n",
       "   - The capital of the USA was established in the late 18th century after the American Revolution. The site was chosen to appease both northern and southern states.\n",
       "\n",
       "4. **Current Capital**:\n",
       "   - The current capital of the United States of America is **Washington, D.C.**.\n",
       "\n",
       "### Conclusion:\n",
       "To directly answer the question: \n",
       "\n",
       "- The capital of the United States of America is **Washington, D.C.**\n",
       "\n",
       "This answer not only provides the information requested but also contextualizes its significance. \n",
       "\n",
       "### Additional Information:\n",
       "- **Washington, D.C.** stands for the District of Columbia and is a federal district, meaning it is not part of any state.\n",
       "- It was named after George Washington, the first President of the United States.\n",
       "\n",
       "Let me know if you have further questions or need more information!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tutor(question=question, MODEL=MODEL_GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ce27d",
   "metadata": {},
   "source": [
    "# Double LLM Collaboration Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3cbb88d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_with_ollama(raw_question):\n",
    "    \"\"\"\n",
    "    Use Ollama to refine the user's raw question into a well-structured prompt.\n",
    "    \"\"\"\n",
    "    print(\"Refining the query using Ollama...\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Refine and structure the following user input.\"},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": raw_question},\n",
    "    ]\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=messages,\n",
    "        stream=False  # Non-streamed refinement\n",
    "    )\n",
    "    refined_query = response.choices[0].message.content\n",
    "    return refined_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "51af91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_ollama_and_gpt(raw_question):\n",
    "    \"\"\"\n",
    "    Use Ollama to refine the user query and GPT to generate the final response.\n",
    "    \"\"\"\n",
    "    # Step 1: Refine the query using Ollama\n",
    "    refined_query = refine_with_ollama(raw_question)\n",
    "    \n",
    "    # Step 2: Generate final response with GPT\n",
    "    print(\"Generating the final response using GPT...\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": refined_query},\n",
    "    ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=messages,\n",
    "        stream=True  # Stream response for dynamic display\n",
    "    )\n",
    "\n",
    "    # Step 3: Combine responses\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(f\"### Refined Query:\\n\\n{refined_query}\\n\\n---\\n\\n### Final Response:\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response_chunk = getattr(chunk.choices[0].delta, \"content\", \"\")\n",
    "        if response_chunk:\n",
    "            response += response_chunk\n",
    "            update_display(Markdown(f\"### Refined Query:\\n\\n{refined_query}\\n\\n---\\n\\n### Final Response:\\n\\n{response}\"), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f5fcc716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "question = \"\"\"\n",
    "Please answer the question and explain it beautifully:\n",
    "who is the founder of Technical Universtiy of Darmstadt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d23d3e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refining the query using Ollama...\n",
      "Generating the final response using GPT...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Refined Query:\n",
       "\n",
       "A straightforward question!\n",
       "\n",
       "The founder of the Technical University of Darmstadt (Technische Universität Darmstadt, or TU Darmstadt) is Bernhard Heine.\n",
       "\n",
       "However, I can provide more context and information about Bernhard Heine. He was a German engineer and politician who played a significant role in the founding of the Technische Hochschule Darmstadt, which later became the modern-day Technische Universität Darmstadt. Founded in 1877, it is one of the oldest technical universities in Germany.\n",
       "\n",
       "Bernhard Heine was born on May 5, 1819, and died on March 12, 1900. Throughout his life, he worked tirelessly to establish a technical university that would provide students with advanced education in fields such as engineering, physics, and mathematics. Under his leadership, the Technische Hochschule Darmstadt grew to become one of Germany's premier technical universities.\n",
       "\n",
       "Today, the Technische Universität Darmstadt is recognized for its research excellence and academic programs in a wide range of disciplines, including engineering, computer science, natural sciences, and humanities.\n",
       "\n",
       "---\n",
       "\n",
       "### Final Response:\n",
       "\n",
       "Certainly! Here’s a concise summary of Bernhard Heine and the Technische Universität Darmstadt, broken down into clear steps:\n",
       "\n",
       "### 1. **Founding Background**\n",
       "   - **Founder:** Bernhard Heine\n",
       "   - **Founded:** 1877\n",
       "   - **Original Name:** Technische Hochschule Darmstadt\n",
       "\n",
       "### 2. **Bernhard Heine**\n",
       "   - **Birth Date:** May 5, 1819\n",
       "   - **Death Date:** March 12, 1900\n",
       "   - **Profession:** German engineer and politician\n",
       "   - **Significance:** Instrumental in establishing a technical university focused on advanced education.\n",
       "\n",
       "### 3. **University Evolution**\n",
       "   - **Growth:** Under Heine's leadership, the university evolved into a major institution for engineering, physics, and mathematics.\n",
       "   - **Modern Recognition:** It is now known as Technische Universität Darmstadt and is recognized for its academic excellence.\n",
       "\n",
       "### 4. **Academic Focus**\n",
       "   - **Disciplines Offered:**\n",
       "     - Engineering\n",
       "     - Computer Science\n",
       "     - Natural Sciences\n",
       "     - Humanities\n",
       "\n",
       "### 5. **Current Standing**\n",
       "   - **Reputation:** TU Darmstadt is esteemed for its research and educational programs, continuing the legacy initiated by Bernhard Heine.\n",
       "\n",
       "### Example of Information Representation\n",
       "Here’s a summarized example using a bullet point format:\n",
       "\n",
       "- **Founder:** Bernhard Heine\n",
       "- **Establishment Year:** 1877\n",
       "- **Current Name:** Technische Universität Darmstadt\n",
       "- **Key Fields:** Engineering, Computer Science, Natural Sciences, Humanities\n",
       "\n",
       "### Additional Note\n",
       "The impact of Bernhard Heine endures, as TU Darmstadt remains a pivotal institution in Germany's educational landscape, inspiring generations of students and researchers alike."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_with_ollama_and_gpt(raw_question=question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
